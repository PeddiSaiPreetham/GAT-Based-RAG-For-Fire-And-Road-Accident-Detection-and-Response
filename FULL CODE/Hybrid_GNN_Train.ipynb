{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "!pip install ultralytics torch torchvision torch-geometric opencv-python tqdm google-generativeai sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset download\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "CLASSES = [\"fire\", \"accident\", \"normal\"]\n",
    "\n",
    "BASE = Path(\"data/gnn_images\")\n",
    "BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for c in CLASSES:\n",
    "    (BASE / c).mkdir(exist_ok=True)\n",
    "\n",
    "def move_images(src_glob, dst, limit=None):\n",
    "    files = glob.glob(src_glob, recursive=True)\n",
    "    if limit:\n",
    "        files = files[:limit]\n",
    "\n",
    "    count = 0\n",
    "    for f in files:\n",
    "        ext = os.path.splitext(f)[-1].lower()\n",
    "        if ext not in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            continue\n",
    "\n",
    "        new_name = f\"{dst}_{count}{ext}\"\n",
    "        shutil.copy(f, BASE / dst / new_name)\n",
    "        count += 1\n",
    "\n",
    "    print(f\"âœ” {count} images added â†’ {dst}\")\n",
    "\n",
    "\n",
    "print(\"\\nâ¬‡ Downloading FIRE dataset...\")\n",
    "fire_path = kagglehub.dataset_download(\"phylake1337/fire-dataset\")\n",
    "\n",
    "move_images(f\"{fire_path}/fire_dataset/fire_images/*.*\", \"fire\")\n",
    "\n",
    "# Non fire = normal class\n",
    "move_images(f\"{fire_path}/fire_dataset/non_fire_images/*.*\", \"normal\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nâ¬‡ Downloading ACCIDENT dataset...\")\n",
    "acc_path = kagglehub.dataset_download(\"ckay16/accident-detection-from-cctv-footage\")\n",
    "\n",
    "# Accident images\n",
    "move_images(f\"{acc_path}/data/train/Accident/*.*\", \"accident\")\n",
    "move_images(f\"{acc_path}/data/val/Accident/*.*\", \"accident\")\n",
    "move_images(f\"{acc_path}/data/test/Accident/*.*\", \"accident\")\n",
    "\n",
    "# Non-Accident â†’ Normal class\n",
    "move_images(f\"{acc_path}/data/train/Non Accident/*.*\", \"normal\")\n",
    "move_images(f\"{acc_path}/data/val/Non Accident/*.*\", \"normal\")\n",
    "move_images(f\"{acc_path}/data/test/Non Accident/*.*\", \"normal\")\n",
    "\n",
    "\n",
    "print(\"\\nâ¬‡ Downloading Natural Images dataset...\")\n",
    "nat_path = kagglehub.dataset_download(\"prasunroy/natural-images\")\n",
    "\n",
    "move_images(f\"{nat_path}/**/*.*\", \"normal\", limit=800)\n",
    "\n",
    "print(\"\\nDataset ready under data/gnn_images/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] DEVICE: cuda\n",
      "[DATASET] Loaded 2168 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== START TRAINING =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:12<00:00, 29.72it/s, loss=0.34, acc=0.833] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1] Loss=0.3396  Acc=0.8335\n",
      "[INFO] Saved best checkpoint (0.8335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:09<00:00, 31.13it/s, loss=0.191, acc=0.922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 2] Loss=0.1907  Acc=0.9220\n",
      "[INFO] Saved best checkpoint (0.9220)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:11<00:00, 30.13it/s, loss=0.183, acc=0.923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 3] Loss=0.1835  Acc=0.9234\n",
      "[INFO] Saved best checkpoint (0.9234)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:11<00:00, 30.26it/s, loss=0.161, acc=0.922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 4] Loss=0.1607  Acc=0.9220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:09<00:00, 31.12it/s, loss=0.152, acc=0.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 5] Loss=0.1520  Acc=0.9299\n",
      "[INFO] Saved best checkpoint (0.9299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:06<00:00, 32.58it/s, loss=0.115, acc=0.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 6] Loss=0.1148  Acc=0.9502\n",
      "[INFO] Saved best checkpoint (0.9502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:10<00:00, 30.65it/s, loss=0.114, acc=0.955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 7] Loss=0.1141  Acc=0.9553\n",
      "[INFO] Saved best checkpoint (0.9553)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:13<00:00, 29.49it/s, loss=0.105, acc=0.96] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 8] Loss=0.1053  Acc=0.9599\n",
      "[INFO] Saved best checkpoint (0.9599)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:09<00:00, 31.03it/s, loss=0.128, acc=0.955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 9] Loss=0.1278  Acc=0.9553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:10<00:00, 30.81it/s, loss=0.0877, acc=0.962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 10] Loss=0.0877  Acc=0.9622\n",
      "[INFO] Saved best checkpoint (0.9622)\n",
      "\n",
      "===== TRAINING COMPLETE =====\n",
      "Best accuracy = 0.9621770977973938\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hybrid GNN Training Script\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Silence ultralytics logs\n",
    "try:\n",
    "    from ultralytics.utils import LOGGER as UL_LOGGER\n",
    "    UL_LOGGER.setLevel(40)\n",
    "except:\n",
    "    pass\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/gnn_images\"\n",
    "CLASSES = [\"normal\", \"fire\", \"accident\"]\n",
    "CLASS_MAP = {c: i for i, c in enumerate(CLASSES)}\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "YOLO_WEIGHTS = \"yolov8n.pt\"\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 1\n",
    "CHECKPOINT_PATH = \"hybrid_gnn_checkpoint.pt\"\n",
    "\n",
    "GLOBAL_DIM = 128\n",
    "LOCAL_DIM = 5\n",
    "IN_DIM = GLOBAL_DIM + LOCAL_DIM  # 133\n",
    "\n",
    "print(\"[CONFIG] DEVICE:\", DEVICE)\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "class GNNImageDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.samples = []\n",
    "        for cname in CLASSES:\n",
    "            folder = os.path.join(root, cname)\n",
    "            imgs = glob.glob(folder + \"/*\")\n",
    "            for f in imgs:\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.samples.append((f, CLASS_MAP[cname]))\n",
    "\n",
    "        print(f\"[DATASET] Loaded {len(self.samples)} images.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_np = np.array(img).astype(\"uint8\")\n",
    "\n",
    "        if img_np.ndim == 4:\n",
    "            img_np = img_np.squeeze()\n",
    "        if img_np.ndim == 2:\n",
    "            img_np = np.stack([img_np]*3, axis=-1)\n",
    "        if img_np.shape[-1] != 3:\n",
    "            img_np = img_np[..., :3]\n",
    "\n",
    "        return img_np, label, path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "# Load ResNet Backbone\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.to(DEVICE)\n",
    "resnet.eval()\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "compress = nn.Linear(512, GLOBAL_DIM).to(DEVICE)\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# YOLO inference\n",
    "def yolo_infer(model, img_np):\n",
    "    pil = Image.fromarray(img_np)\n",
    "    with torch.no_grad():\n",
    "        out = model.predict(pil, verbose=False, device=DEVICE)\n",
    "    return out[0]\n",
    "\n",
    "\n",
    "# Build Graph\n",
    "def build_graph(image_np, yres):\n",
    "    H, W = image_np.shape[:2]\n",
    "\n",
    "    # global embedding\n",
    "    with torch.no_grad():\n",
    "        t = img_transform(Image.fromarray(image_np)).unsqueeze(0).to(DEVICE)\n",
    "        feat = resnet(t)\n",
    "    global_vec = compress(feat).squeeze(0)\n",
    "\n",
    "    boxes = getattr(yres, \"boxes\", None)\n",
    "\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        local = torch.zeros((LOCAL_DIM,), device=DEVICE)\n",
    "        x = torch.cat([global_vec, local]).unsqueeze(0)\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "        return Data(x=x.float(), edge_index=edge_index)\n",
    "\n",
    "    xyxy = boxes.xyxy.cpu().numpy()\n",
    "    conf = boxes.conf.cpu().numpy()\n",
    "    cls_ids = boxes.cls.cpu().numpy()\n",
    "\n",
    "    nodes = []\n",
    "    for i in range(len(xyxy)):\n",
    "        x1,y1,x2,y2 = xyxy[i]\n",
    "        cx = (x1 + x2) / 2 / W\n",
    "        cy = (y1 + y2) / 2 / H\n",
    "        area = ((x2 - x1)*(y2 - y1)) / (W*H)\n",
    "\n",
    "        local_feat = torch.tensor([cls_ids[i], conf[i], cx, cy, area], device=DEVICE)\n",
    "        full_vec = torch.cat([global_vec, local_feat])\n",
    "        nodes.append(full_vec)\n",
    "\n",
    "    x = torch.stack(nodes)\n",
    "    N = x.size(0)\n",
    "    if N == 1:\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "    else:\n",
    "        edges = [[i,j] for i in range(N) for j in range(N) if i != j]\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long, device=DEVICE).t()\n",
    "\n",
    "    return Data(x=x.float(), edge_index=edge_index)\n",
    "\n",
    "\n",
    "# Model\n",
    "class HybridGAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(IN_DIM, 256, heads=2)       # -> 512\n",
    "        self.conv2 = GATConv(512, 256, heads=1)          # -> 256\n",
    "        self.fc = nn.Linear(256, len(CLASSES))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = x.mean(dim=0, keepdim=True)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "yolo = YOLO(YOLO_WEIGHTS)\n",
    "yolo.to(DEVICE)\n",
    "\n",
    "dataset = GNNImageDataset(DATA_DIR)\n",
    "\n",
    "labels = [lbl for _, lbl, _ in dataset]\n",
    "counts = Counter(labels)\n",
    "weights = torch.DoubleTensor([1.0/np.sqrt(counts[lbl]) for lbl in labels])\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "loader = DataLoader(dataset, batch_size=1, sampler=sampler, num_workers=2)\n",
    "\n",
    "model = HybridGAT().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(compress.parameters()),\n",
    "    lr=LR\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "print(\"\\n===== START TRAINING =====\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for img_np, label, path in pbar:\n",
    "        img_np = img_np[0].numpy()\n",
    "\n",
    "        yres = yolo_infer(yolo, img_np)\n",
    "\n",
    "        graph = build_graph(img_np, yres).to(DEVICE)\n",
    "\n",
    "        lbl = torch.tensor([label], device=DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(logits, lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = logits.argmax(1).item()\n",
    "        correct += (pred == label)\n",
    "        total += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": float(total_loss / total),\n",
    "            \"acc\": float(correct / total)\n",
    "        })\n",
    "\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"[EPOCH {epoch}] Loss={float(total_loss/total):.4f}  Acc={float(acc):.4f}\")\n",
    "\n",
    "    if float(acc) > best_acc:\n",
    "        best_acc = float(acc)\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"compress_state\": compress.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_acc\": best_acc,\n",
    "            \"classes\": CLASSES\n",
    "        }, CHECKPOINT_PATH)\n",
    "        print(f\"[INFO] Saved best checkpoint ({best_acc:.4f})\")\n",
    "\n",
    "\n",
    "print(\"\\n===== TRAINING COMPLETE =====\")\n",
    "print(\"Best accuracy =\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVAL] Loaded 2168 images.\n",
      "\n",
      "[EVAL] Running evaluation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [01:14<00:00, 29.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RESULTS =====\n",
      "Accuracy : 0.9746309963099631\n",
      "Precision: 0.9626976724178463\n",
      "Recall   : 0.9719262997227673\n",
      "F1 Score : 0.9671089083058789\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1007    1   36]\n",
      " [   0  755    0]\n",
      " [  18    0  351]]\n"
     ]
    }
   ],
   "source": [
    "# Hybrid GNN Evaluation\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "DATA_DIR = \"data/gnn_images\"\n",
    "CHECKPOINT_PATH = \"hybrid_gnn_checkpoint.pt\"\n",
    "CLASSES = [\"normal\", \"fire\", \"accident\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GLOBAL_DIM = 128\n",
    "LOCAL_DIM = 5\n",
    "IN_DIM = GLOBAL_DIM + LOCAL_DIM\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class EvalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.samples = []\n",
    "        for cname in CLASSES:\n",
    "            folder = os.path.join(root, cname)\n",
    "            imgs = glob.glob(folder + \"/*\")\n",
    "            for f in imgs:\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.samples.append((f, CLASSES.index(cname)))\n",
    "\n",
    "        print(f\"[EVAL] Loaded {len(self.samples)} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_np = np.array(img).astype(\"uint8\")\n",
    "        return img_np, label, path\n",
    "\n",
    "\n",
    "# Backbones\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet.to(DEVICE)\n",
    "resnet.eval()\n",
    "for p in resnet.parameters(): p.requires_grad = False\n",
    "\n",
    "compress = nn.Linear(512, GLOBAL_DIM).to(DEVICE)\n",
    "compress.eval()\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Silent YOLO\n",
    "yolo = YOLO(\"yolov8n.pt\")\n",
    "yolo.to(DEVICE)\n",
    "\n",
    "def yolo_infer(img_np):\n",
    "    pil = Image.fromarray(img_np)\n",
    "    with torch.no_grad():\n",
    "        return yolo.predict(pil, verbose=False, device=DEVICE)[0]\n",
    "\n",
    "\n",
    "# Graph builder (same as training)\n",
    "def build_graph(image_np, yres):\n",
    "    H, W = image_np.shape[:2]\n",
    "    with torch.no_grad():\n",
    "        t = img_transform(Image.fromarray(image_np)).unsqueeze(0).to(DEVICE)\n",
    "        feat = resnet(t)\n",
    "    global_vec = compress(feat).squeeze(0)\n",
    "\n",
    "    boxes = getattr(yres, \"boxes\", None)\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        local = torch.zeros((LOCAL_DIM,), device=DEVICE)\n",
    "        x = torch.cat([global_vec, local]).unsqueeze(0)\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "        return Data(x=x.float(), edge_index=edge_index)\n",
    "\n",
    "    xyxy = boxes.xyxy.cpu().numpy()\n",
    "    conf = boxes.conf.cpu().numpy()\n",
    "    cls_ids = boxes.cls.cpu().numpy()\n",
    "\n",
    "    nodes = []\n",
    "    for i in range(len(xyxy)):\n",
    "        x1,y1,x2,y2 = xyxy[i]\n",
    "        cx = (x1+x2)/2/W\n",
    "        cy = (y1+y2)/2/H\n",
    "        area = ((x2-x1)*(y2-y1))/(W*H)\n",
    "\n",
    "        local = torch.tensor([cls_ids[i], conf[i], cx, cy, area], device=DEVICE)\n",
    "        node = torch.cat([global_vec, local])\n",
    "        nodes.append(node)\n",
    "\n",
    "    x = torch.stack(nodes).float()\n",
    "\n",
    "    N = x.size(0)\n",
    "    if N == 1:\n",
    "        edge_index = torch.tensor([[0],[0]], device=DEVICE)\n",
    "    else:\n",
    "        edges = [[i,j] for i in range(N) for j in range(N) if i != j]\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long, device=DEVICE).t()\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "\n",
    "# GNN Model\n",
    "class HybridGAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(IN_DIM, 256, heads=2)\n",
    "        self.conv2 = GATConv(512, 256, heads=1)\n",
    "        self.fc = nn.Linear(256, len(CLASSES))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = x.mean(dim=0, keepdim=True)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "\n",
    "model = HybridGAT().to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "\n",
    "compress.load_state_dict(ckpt[\"compress_state\"])\n",
    "\n",
    "model.eval()\n",
    "compress.eval()\n",
    "\n",
    "\n",
    "# Evaluation loop\n",
    "dataset = EvalDataset(DATA_DIR)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"\\n[EVAL] Running evaluation...\\n\")\n",
    "\n",
    "for img_np, label, path in tqdm(loader):\n",
    "    img_np = img_np[0].numpy()\n",
    "    label = int(label)\n",
    "\n",
    "    yres = yolo_infer(img_np)\n",
    "    graph = build_graph(img_np, yres).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph.x, graph.edge_index)\n",
    "        pred = logits.argmax().item()\n",
    "\n",
    "    y_true.append(label)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\n===== RESULTS =====\")\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1 Score :\", f1)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded at: /teamspace/studios/this_studio/.cache/kagglehub/datasets/smrutisanchitadas/flame-dataset-fire-classification/versions/2\n",
      "Copying Fire images...\n",
      "Copying No_Fire images...\n",
      "DONE. Evaluation dataset created at: data/eval_fire/train\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Download the dataset\n",
    "path = kagglehub.dataset_download(\"smrutisanchitadas/flame-dataset-fire-classification\")\n",
    "print(\"Dataset downloaded at:\", path)\n",
    "\n",
    "test_root = os.path.join(path, \"Training\", \"Training\")\n",
    "\n",
    "src_fire = os.path.join(test_root, \"Fire\")\n",
    "src_non_fire = os.path.join(test_root, \"No_Fire\")\n",
    "\n",
    "dst_root = \"data/eval_fire/train\"\n",
    "dst_fire = os.path.join(dst_root, \"fire\")\n",
    "dst_non_fire = os.path.join(dst_root, \"non_fire\")\n",
    "\n",
    "os.makedirs(dst_fire, exist_ok=True)\n",
    "os.makedirs(dst_non_fire, exist_ok=True)\n",
    "\n",
    "print(\"Copying Fire images...\")\n",
    "for f in glob.glob(src_fire + \"/*\"):\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        shutil.copy(f, dst_fire)\n",
    "\n",
    "print(\"Copying No_Fire images...\")\n",
    "for f in glob.glob(src_non_fire + \"/*\"):\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        shutil.copy(f, dst_non_fire)\n",
    "\n",
    "print(\"DONE. Evaluation dataset created at:\", dst_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cuda\n",
      "[INFO] Model loaded successfully.\n",
      "[EVAL] Loaded 8617 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8617/8617 [02:07<00:00, 67.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RESULTS =====\n",
      "Accuracy : 0.4068701404200998\n",
      "Precision: 1.0\n",
      "Recall   : 0.005061319836480436\n",
      "F1 Score : 0.010071663761379043\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3480    0]\n",
      " [5111   26]]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#   FIXED HYBRID GNN EVALUATION FOR FIRE/NON_FIRE DATASET\n",
    "#   (Checkpoint trained on 3 classes â†’ normal, fire, accident)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "TEST_DIR = \"data/eval_fire\"  # fire / non_fire\n",
    "CHECKPOINT_PATH = \"hybrid_gnn_checkpoint.pt\"\n",
    "\n",
    "TRAIN_CLASSES = [\"normal\", \"fire\", \"accident\"]   # original model classes\n",
    "EVAL_CLASSES  = [\"non_fire\", \"fire\"]             # evaluation classes\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "GLOBAL_DIM = 128\n",
    "LOCAL_DIM = 5\n",
    "IN_DIM = GLOBAL_DIM + LOCAL_DIM\n",
    "\n",
    "print(\"DEVICE =\", DEVICE)\n",
    "\n",
    "\n",
    "# -------------- Dataset ------------------\n",
    "class EvalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.samples = []\n",
    "        for cname, label in [(\"fire\",1), (\"non_fire\",0)]:\n",
    "            folder = os.path.join(root, cname)\n",
    "            imgs = glob.glob(folder + \"/*\")\n",
    "            for f in imgs:\n",
    "                if f.lower().endswith((\".jpg\",\".png\",\".jpeg\")):\n",
    "                    self.samples.append((f, label))\n",
    "\n",
    "        print(f\"[EVAL] Loaded {len(self.samples)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_np = np.array(img).astype(\"uint8\")\n",
    "        return img_np, label, path\n",
    "\n",
    "\n",
    "# ------------- Backbone ResNet18 -------------\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.to(DEVICE).eval()\n",
    "\n",
    "for p in resnet.parameters(): \n",
    "    p.requires_grad = False\n",
    "\n",
    "compress = nn.Linear(512, GLOBAL_DIM).to(DEVICE).eval()\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ------------- YOLO8n silent -------------------\n",
    "yolo = YOLO(\"yolov8n.pt\")\n",
    "yolo.to(DEVICE)\n",
    "\n",
    "def yolo_infer(img_np):\n",
    "    pil = Image.fromarray(img_np)\n",
    "    with torch.no_grad():\n",
    "        return yolo.predict(pil, verbose=False, device=DEVICE)[0]\n",
    "\n",
    "\n",
    "# -------------- Graph builder ------------------\n",
    "def build_graph(image_np, yres):\n",
    "    H, W = image_np.shape[:2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t = img_transform(Image.fromarray(image_np)).unsqueeze(0).to(DEVICE)\n",
    "        feat = resnet(t)\n",
    "    gvec = compress(feat).squeeze(0)\n",
    "\n",
    "    boxes = getattr(yres, \"boxes\", None)\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        local = torch.zeros((LOCAL_DIM,), device=DEVICE)\n",
    "        x = torch.cat([gvec, local]).unsqueeze(0)\n",
    "        ei = torch.tensor([[0],[0]], device=DEVICE)\n",
    "        return Data(x=x.float(), edge_index=ei)\n",
    "\n",
    "    xyxy = boxes.xyxy.cpu().numpy()\n",
    "    conf = boxes.conf.cpu().numpy()\n",
    "    cls_ids = boxes.cls.cpu().numpy()\n",
    "\n",
    "    nodes=[]\n",
    "    for i in range(len(xyxy)):\n",
    "        x1,y1,x2,y2 = xyxy[i]\n",
    "        cx=(x1+x2)/2/W\n",
    "        cy=(y1+y2)/2/H\n",
    "        area=((x2-x1)*(y2-y1))/(W*H)\n",
    "\n",
    "        local = torch.tensor([cls_ids[i], conf[i], cx, cy, area],device=DEVICE)\n",
    "        nodes.append(torch.cat([gvec, local]))\n",
    "\n",
    "    x = torch.stack(nodes)\n",
    "    N = x.size(0)\n",
    "\n",
    "    if N==1:\n",
    "        ei = torch.tensor([[0],[0]], device=DEVICE)\n",
    "    else:\n",
    "        edges = [[i,j] for i in range(N) for j in range(N) if i!=j]\n",
    "        ei = torch.tensor(edges,device=DEVICE).t()\n",
    "\n",
    "    return Data(x=x.float(), edge_index=ei)\n",
    "\n",
    "\n",
    "# ---------------- Hybrid GAT model (3 output classes) ----------------\n",
    "class HybridGAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(IN_DIM, 256, heads=2)\n",
    "        self.conv2 = GATConv(512, 256, heads=1)\n",
    "        self.fc = nn.Linear(256, 3)  # TRAINED ON 3 CLASSES\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = x.mean(dim=0, keepdim=True)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ---------------- Load checkpoint ----------------\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "\n",
    "model = HybridGAT().to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "compress.load_state_dict(ckpt[\"compress_state\"])\n",
    "\n",
    "model.eval()\n",
    "compress.eval()\n",
    "\n",
    "print(\"[INFO] Model loaded successfully.\")\n",
    "\n",
    "\n",
    "# ---------------- Evaluation ----------------\n",
    "dataset = EvalDataset(TEST_DIR)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "\n",
    "for img_np, label, path in tqdm(loader):\n",
    "    img_np = img_np[0].numpy()\n",
    "    label = int(label)\n",
    "\n",
    "    yres = yolo_infer(img_np)\n",
    "    graph = build_graph(img_np,yres).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph.x, graph.edge_index)\n",
    "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "    # map 3 -> 2 classes\n",
    "    fire_prob = probs[1]                       # class 1 (fire)\n",
    "    nonfire_prob = max(probs[0], probs[2])     # normal or accident â†’ non_fire\n",
    "\n",
    "    pred = 1 if fire_prob > nonfire_prob else 0\n",
    "\n",
    "    y_true.append(label)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "\n",
    "# ---------------- Metrics ----------------\n",
    "acc = accuracy_score(y_true,y_pred)\n",
    "prec,rec,f1,_ = precision_recall_fscore_support(y_true,y_pred,average=\"binary\")\n",
    "cm = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "print(\"\\n===== RESULTS =====\")\n",
    "print(\"Accuracy :\",acc)\n",
    "print(\"Precision:\",prec)\n",
    "print(\"Recall   :\",rec)\n",
    "print(\"F1 Score :\",f1)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] DEVICE: cuda\n",
      "[DATASET] Loaded 39375 images.\n",
      "\n",
      "===== START TRAINING =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39375/39375 [10:41<00:00, 61.42it/s, loss=0.032, acc=0.989] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1] Loss=0.0320  Acc=0.9886\n",
      "[INFO] Saved best model 0.9886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39375/39375 [10:28<00:00, 62.64it/s, loss=0.0157, acc=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 2] Loss=0.0157  Acc=0.9949\n",
      "[INFO] Saved best model 0.9949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  21%|â–ˆâ–ˆâ–       | 8457/39375 [02:15<08:18, 61.99it/s, loss=0.0147, acc=0.996] "
     ]
    }
   ],
   "source": [
    "# Hybrid GNN Training Script (Fire vs Non-Fire)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Silence ultralytics logs\n",
    "try:\n",
    "    from ultralytics.utils import LOGGER as UL_LOGGER\n",
    "    UL_LOGGER.setLevel(40)\n",
    "except:\n",
    "    pass\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# CONFIG\n",
    "DATA_DIR = \"data/eval_fire/train\"   # ðŸ”¥ NEW FOLDER\n",
    "CLASSES = [\"non_fire\", \"fire\"]      # ðŸ”¥ ONLY 2 CLASSES NOW\n",
    "CLASS_MAP = {c: i for i, c in enumerate(CLASSES)}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "YOLO_WEIGHTS = \"yolov8n.pt\"\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# ðŸ”¥ NEW CHECKPOINT NAME\n",
    "CHECKPOINT_PATH = \"hybrid_fire_checkpoint.pt\"\n",
    "\n",
    "GLOBAL_DIM = 128\n",
    "LOCAL_DIM = 5\n",
    "IN_DIM = GLOBAL_DIM + LOCAL_DIM\n",
    "\n",
    "print(\"[CONFIG] DEVICE:\", DEVICE)\n",
    "\n",
    "\n",
    "# DATASET\n",
    "class GNNImageDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.samples = []\n",
    "        for cname in CLASSES:\n",
    "            folder = os.path.join(root, cname)\n",
    "            imgs = glob.glob(folder + \"/*\")\n",
    "            for f in imgs:\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.samples.append((f, CLASS_MAP[cname]))\n",
    "\n",
    "        print(f\"[DATASET] Loaded {len(self.samples)} images.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_np = np.array(img).astype(\"uint8\")\n",
    "\n",
    "        # fix dimensionality\n",
    "        if img_np.ndim == 4:\n",
    "            img_np = img_np.squeeze()\n",
    "        if img_np.ndim == 2:\n",
    "            img_np = np.stack([img_np]*3, axis=-1)\n",
    "        if img_np.shape[-1] != 3:\n",
    "            img_np = img_np[..., :3]\n",
    "\n",
    "        return img_np, label, path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "# RESNET BACKBONE\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.to(DEVICE).eval()\n",
    "for p in resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "compress = nn.Linear(512, GLOBAL_DIM).to(DEVICE)\n",
    "\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# YOLO inference\n",
    "def yolo_infer(model, img_np):\n",
    "    pil = Image.fromarray(img_np)\n",
    "    with torch.no_grad():\n",
    "        out = model.predict(pil, verbose=False, device=DEVICE)\n",
    "    return out[0]\n",
    "\n",
    "\n",
    "# GRAPH BUILDER\n",
    "def build_graph(image_np, yres):\n",
    "    \"\"\"\n",
    "    Fully safe graph builder.\n",
    "    Ensures:\n",
    "      - x has shape (N, IN_DIM)\n",
    "      - edge_index ALWAYS has shape (2, X) with X>=1\n",
    "      - never returns empty edge_index\n",
    "    \"\"\"\n",
    "    H, W = image_np.shape[:2]\n",
    "\n",
    "    # 1) GLOBAL FEATURE (ResNet18 â†’ compress)\n",
    "    try:\n",
    "        t = img_transform(Image.fromarray(image_np)).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            feat = resnet(t)\n",
    "        global_vec = compress(feat).squeeze(0)\n",
    "    except:\n",
    "        # catastrophic fallback\n",
    "        global_vec = torch.zeros(GLOBAL_DIM, device=DEVICE)\n",
    "\n",
    "    # 2) YOLO DETECTIONS\n",
    "    boxes = getattr(yres, \"boxes\", None)\n",
    "\n",
    "    # ----------- CASE A: NO DETECTIONS ---------------\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        x = torch.cat([global_vec, torch.zeros(LOCAL_DIM, device=DEVICE)], dim=0)\n",
    "        x = x.unsqueeze(0)  # shape (1, IN_DIM)\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "        return Data(x=x.float(), edge_index=edge_index)\n",
    "\n",
    "    # Try to extract detection arrays safely\n",
    "    try:\n",
    "        xyxy = boxes.xyxy.cpu().numpy()\n",
    "        conf = boxes.conf.cpu().numpy()\n",
    "        cls_ids = boxes.cls.cpu().numpy()\n",
    "    except:\n",
    "        # fallback to dummy graph\n",
    "        x = torch.cat([global_vec, torch.zeros(LOCAL_DIM, device=DEVICE)], dim=0)\n",
    "        x = x.unsqueeze(0)\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "        return Data(x=x.float(), edge_index=edge_index)\n",
    "\n",
    "    # 3) Build node features\n",
    "    nodes = []\n",
    "    for i in range(len(xyxy)):\n",
    "        x1, y1, x2, y2 = xyxy[i]\n",
    "        cx = (x1 + x2) / 2 / max(W, 1)\n",
    "        cy = (y1 + y2) / 2 / max(H, 1)\n",
    "        area = ((x2 - x1) * (y2 - y1)) / max(W*H, 1)\n",
    "\n",
    "        local = torch.tensor(\n",
    "            [float(cls_ids[i]), float(conf[i]), float(cx), float(cy), float(area)],\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        nodes.append(torch.cat([global_vec, local], dim=0))\n",
    "\n",
    "    # If failed to build nodes for some reason\n",
    "    if len(nodes) == 0:\n",
    "        x = torch.cat([global_vec, torch.zeros(LOCAL_DIM, device=DEVICE)], dim=0).unsqueeze(0)\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "        return Data(x=x.float(), edge_index=edge_index)\n",
    "\n",
    "    # stack\n",
    "    x = torch.stack(nodes).float()\n",
    "    N = x.size(0)\n",
    "-\n",
    "    # 4) Build edges safely\n",
    "    if N == 1:\n",
    "        # self-loop only\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "    else:\n",
    "        # fully connected\n",
    "        edges = [[i, j] for i in range(N) for j in range(N) if i != j]\n",
    "        if len(edges) == 0:\n",
    "            edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "        else:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long, device=DEVICE).t().contiguous()\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "\n",
    "\n",
    "# HYBRID GAT MODEL (2 CLASSES)\n",
    "class HybridGAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(IN_DIM, 256, heads=2)\n",
    "        self.conv2 = GATConv(512, 256, heads=1)\n",
    "        self.fc = nn.Linear(256, 2)   # ðŸ”¥ ONLY 2 CLASSES NOW\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = x.mean(dim=0, keepdim=True)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING INITIALIZATION\n",
    "# ============================================================\n",
    "yolo = YOLO(YOLO_WEIGHTS)\n",
    "yolo.to(DEVICE)\n",
    "\n",
    "dataset = GNNImageDataset(DATA_DIR)\n",
    "labels = [lbl for _, lbl, _ in dataset]\n",
    "counts = Counter(labels)\n",
    "weights = torch.DoubleTensor([1.0/np.sqrt(counts[l]) for l in labels])\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "loader = DataLoader(dataset, batch_size=1, sampler=sampler, num_workers=2)\n",
    "\n",
    "model = HybridGAT().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(compress.parameters()),\n",
    "    lr=LR\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "print(\"\\n===== START TRAINING =====\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for img_np, label, _ in pbar:\n",
    "        img_np = img_np[0].numpy()\n",
    "\n",
    "        yres = yolo_infer(yolo, img_np)\n",
    "        graph = build_graph(img_np, yres).to(DEVICE)\n",
    "\n",
    "        lbl = torch.tensor([label], device=DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(logits, lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = logits.argmax(1).item()\n",
    "        correct += (pred == label)\n",
    "        total += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": float(total_loss/total),\n",
    "            \"acc\": float(correct/total)\n",
    "        })\n",
    "\n",
    "    acc = correct/total\n",
    "    print(f\"[EPOCH {epoch}] Loss={total_loss/total:.4f}  Acc={float(acc):.4f}\")\n",
    "\n",
    "    if float(acc) > best_acc:\n",
    "        best_acc = float(acc)\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"compress_state\": compress.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_acc\": best_acc,\n",
    "            \"classes\": CLASSES\n",
    "        }, CHECKPOINT_PATH)\n",
    "        print(f\"[INFO] Saved best model {best_acc:.4f}\")\n",
    "\n",
    "print(\"\\n===== TRAINING COMPLETE =====\")\n",
    "print(\"Best accuracy:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cuda\n",
      "[INFO] Loaded trained fire model.\n",
      "[EVAL] Loaded 8617 samples.\n",
      "\n",
      "[EVAL] Running evaluation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8617/8617 [01:53<00:00, 76.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL RESULTS =====\n",
      "Accuracy : 0.6749448764071022\n",
      "Precision: 0.6649717514124294\n",
      "Recall   : 0.9164882226980728\n",
      "F1 Score : 0.7707293116149627\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1108 2372]\n",
      " [ 429 4708]]\n"
     ]
    }
   ],
   "source": [
    "#   HYBRID GNN EVALUATION FOR 2-CLASS FIRE MODEL\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "TEST_DIR = \"data/eval_fire\"     # contains fire / non_fire\n",
    "CHECKPOINT_PATH = \"hybrid_fire_checkpoint.pt\"\n",
    "\n",
    "CLASSES = [\"non_fire\", \"fire\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GLOBAL_DIM = 128\n",
    "LOCAL_DIM = 5\n",
    "IN_DIM = GLOBAL_DIM + LOCAL_DIM\n",
    "\n",
    "print(\"DEVICE =\", DEVICE)\n",
    "\n",
    "\n",
    "# ---------------- Dataset ----------------\n",
    "class EvalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.samples = []\n",
    "        for cname, label in [(\"non_fire\",0), (\"fire\",1)]:\n",
    "            folder = os.path.join(root, cname)\n",
    "            imgs = glob.glob(folder + \"/*\")\n",
    "            for f in imgs:\n",
    "                if f.lower().endswith((\".jpg\",\".png\",\".jpeg\")):\n",
    "                    self.samples.append((f, label))\n",
    "        print(f\"[EVAL] Loaded {len(self.samples)} samples.\")\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_np = np.array(img).astype(\"uint8\")\n",
    "        return img_np, label, path\n",
    "\n",
    "\n",
    "# ---------------- RESNET BACKBONE ----------------\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.to(DEVICE).eval()\n",
    "for p in resnet.parameters(): p.requires_grad = False\n",
    "\n",
    "compress = nn.Linear(512, GLOBAL_DIM).to(DEVICE)\n",
    "\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# ---------------- YOLO8n ----------------\n",
    "yolo = YOLO(\"yolov8n.pt\")\n",
    "yolo.to(DEVICE)\n",
    "\n",
    "def yolo_infer(img_np):\n",
    "    pil = Image.fromarray(img_np)\n",
    "    with torch.no_grad():\n",
    "        return yolo.predict(pil, verbose=False, device=DEVICE)[0]\n",
    "\n",
    "\n",
    "# ---------------- Graph Builder ----------------\n",
    "def build_graph(image_np, yres):\n",
    "    H, W = image_np.shape[:2]\n",
    "\n",
    "    # global image feature\n",
    "    with torch.no_grad():\n",
    "        t = img_transform(Image.fromarray(image_np)).unsqueeze(0).to(DEVICE)\n",
    "        feat = resnet(t)\n",
    "    gvec = compress(feat).squeeze(0)\n",
    "\n",
    "    boxes = getattr(yres, \"boxes\", None)\n",
    "\n",
    "    # No detections â†’ 1 dummy node\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        x = torch.cat([gvec, torch.zeros(LOCAL_DIM, device=DEVICE)], dim=0).unsqueeze(0)\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "        return Data(x=x.float(), edge_index=edge_index)\n",
    "\n",
    "    xyxy = boxes.xyxy.cpu().numpy()\n",
    "    conf = boxes.conf.cpu().numpy()\n",
    "    cls_ids = boxes.cls.cpu().numpy()\n",
    "\n",
    "    nodes = []\n",
    "    for i in range(len(xyxy)):\n",
    "        x1,y1,x2,y2 = xyxy[i]\n",
    "        cx = (x1+x2)/2/W\n",
    "        cy = (y1+y2)/2/H\n",
    "        area = ((x2-x1)*(y2-y1))/(W*H)\n",
    "\n",
    "        local = torch.tensor([cls_ids[i], conf[i], cx, cy, area], device=DEVICE)\n",
    "        nodes.append(torch.cat([gvec, local]))\n",
    "\n",
    "    x = torch.stack(nodes)\n",
    "    N = x.size(0)\n",
    "\n",
    "    # fully connected edges\n",
    "    if N == 1:\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "    else:\n",
    "        edges = [[i,j] for i in range(N) for j in range(N) if i!=j]\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long, device=DEVICE).t()\n",
    "\n",
    "    return Data(x=x.float(), edge_index=edge_index)\n",
    "\n",
    "\n",
    "# ---------------- Hybrid GAT (2-class) ----------------\n",
    "class HybridGAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(IN_DIM, 256, heads=2)\n",
    "        self.conv2 = GATConv(512, 256, heads=1)\n",
    "        self.fc = nn.Linear(256, 2)  # FIRE / NON_FIRE\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = x.mean(dim=0, keepdim=True)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ---------------- Load Checkpoint ----------------\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "\n",
    "model = HybridGAT().to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "\n",
    "compress.load_state_dict(ckpt[\"compress_state\"])\n",
    "\n",
    "model.eval()\n",
    "compress.eval()\n",
    "\n",
    "print(\"[INFO] Loaded trained fire model.\")\n",
    "\n",
    "\n",
    "# ---------------- Run Evaluation ----------------\n",
    "dataset = EvalDataset(TEST_DIR)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "\n",
    "print(\"\\n[EVAL] Running evaluation...\\n\")\n",
    "\n",
    "for img_np, label, path in tqdm(loader):\n",
    "    img_np = img_np[0].numpy()\n",
    "\n",
    "    yres = yolo_infer(img_np)\n",
    "    graph = build_graph(img_np, yres).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph.x, graph.edge_index)\n",
    "        pred = logits.argmax().item()\n",
    "\n",
    "    y_true.append(label)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "\n",
    "# ---------------- Metrics ----------------\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\n===== FINAL RESULTS =====\")\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1 Score :\", f1)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
